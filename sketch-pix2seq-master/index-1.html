<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Desconocido </title></head><body>
<h1 id="sketch-pix2seq">sketch-pix2seq</h1>
<p>This is the reimplementation code of paper <a href="https://arxiv.org/pdf/1709.04121.pdf">Sketch-pix2seq: a Model to Generate Sketches of Multiple Categories</a>.</p>
<table>
<thead>
<tr>
<th>Input</th>
<th>Generated examples</th>
</tr>
</thead>
<tbody>
<tr>
<td><img alt="output examples" src="https://github.com/MarkMoHR/sketch-pix2seq/blob/master/assets/1583-sample_gt.png" /></td>
<td><img alt="output examples" src="https://github.com/MarkMoHR/sketch-pix2seq/blob/master/assets/1583-sample_pred_cond_100.svg" /></td>
</tr>
</tbody>
</table>
<h2 id="requirements">Requirements</h2>
<ul>
<li>Python 3</li>
<li>Tensorflow (&gt;= 1.4.0)</li>
<li><a href="https://inkscape.org/">InkScape</a> or <a href="https://cairosvg.org/">CairoSVG</a> (For vector sketch rendering. Choose one of them is ok.)</li>
</ul>
<p><code>sudo apt-get install inkscape
  # or
  pip3 install cairosvg</code></p>
<h2 id="data-preparations">Data Preparations</h2>
<p>Follow these steps:</p>
<ol>
<li>
<p>First download the <code>.npz</code> data from <a href="https://github.com/googlecreativelab/quickdraw-dataset#sketch-rnn-quickdraw-dataset"><em>QuickDraw</em></a> dataset. Choose one or more categories as you like.</p>
</li>
<li>
<p>Place the <code>.npz</code> packages under <code>datasets/npz/</code> dir.</p>
</li>
<li>
<p>Modify the hyper params in <code>get_default_hparams()</code> in <a href="https://github.com/MarkMoHR/sketch-pix2seq/blob/master/model.py">model.py</a></p>
<ul>
<li>Set the name(s) of the downloaded packages in <code>data_set</code></li>
<li>Set the size of the raster image in <code>img_H</code> / <code>img_W</code></li>
</ul>
</li>
<li>
<p>We provide two approaches of rendering sequential data into raster images:</p>
<ul>
<li>Using <a href="https://inkscape.org/">InkScape</a>: this approach is <strong>slower</strong> but <strong>accurate</strong> all the time</li>
</ul>
<p><code>python render_svg2bitmap.py --data_base_dir='datasets' --render_mode='v1'</code></p>
<ul>
<li>Using <a href="https://cairosvg.org/">CairoSVG</a>: this approach is <strong>faster</strong>, but will have one-pixel <strong>misalignment</strong> problem sometimes (when setting image-width to 256, it will turn out to be 255 sometimes)</li>
</ul>
<p><code>python render_svg2bitmap.py --data_base_dir='datasets' --render_mode='v2'</code></p>
<p>The raster images can be found under <code>datasets/</code> dir.</p>
</li>
</ol>
<h2 id="training">Training</h2>
<p>Run this command for default training:</p>
<p><code>python sketch_pix2seq_train.py</code></p>
<p>You can also change the settings, <em>e.g.</em> image size, batch size, in <code>get_default_hparams()</code> in <a href="https://github.com/MarkMoHR/sketch-pix2seq/blob/master/model.py">model.py</a>. For multi-category training, set the <code>data_set</code> with a list of more than one packages' names.</p>
<h3 id="training-procedure">Training procedure</h3>
<p>The following figure shows the <em>reconstruction loss</em> during training within 60k iterations. The orange line belongs to <a href="https://arxiv.org/abs/1704.03477">sketch-rnn</a> and the blue one belongs to sketch-pix2seq.</p>
<p><img alt="loss" src="https://github.com/MarkMoHR/sketch-pix2seq/blob/master/assets/loss-r.png" /></p>
<h2 id="sampling">Sampling</h2>
<p>With trained model placed under <code>outputs/snapshot/</code> dir, run this command for sampling:</p>
<p><code>python sketch_pix2seq_sampling.py</code></p>
<p>And results in <code>.svg</code> format can be found under <code>outputs/sampling/</code> dir.</p>
<h2 id="credits">Credits</h2>
<ul>
<li>This code is modified from repo of <a href="https://github.com/tensorflow/magenta/tree/master/magenta/models/sketch_rnn">Sketch-RNN</a>.</li>
</ul>
</body></html>